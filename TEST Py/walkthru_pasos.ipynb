{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#good to know\n",
    "#import sys\n",
    "#print('Python version ' + sys.version)\n",
    "#print('Pandas version: ' + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import basic py library\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# then import the plotting fx\n",
    "\n",
    "#from scipy import stats,integrate\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#NEED plt parameters foor graphic output!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name and read the excel file\n",
    "# xx=pd.read_excel('C:/Users/mjc341/desktop/folder_location/name_of_file.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enter name of file for example xx and will display the entire dateframe but fit to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now import seaborn for viz\n",
    "# import seaborn as sns\n",
    "# sns.set (color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read csv the same way\n",
    "#import csv\n",
    "# xx2=pd.read_csv(\"http://pythonhow.com/wp-content/uploads/2016/01/Income_data.csv\") This is a sample file online\n",
    "# enter xx2 to see the dataframe\n",
    "#Have to save as csv DOS file not excel csv or PY cant read it, Save downloaded csv files as csv DOS\n",
    "# to set an index as a key value\n",
    "# make the applicant ID the key value\n",
    "#xxindexed=xx.set_index('name-of-col') for example if empl ID will be the index or key value\n",
    "# make the applicant ID the key value\n",
    "#dfindexed=df2.set_index('APPLICANT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CLEAN YOUR DATA CREATE a na values list and place it before the read statement ALSO Change numbers that are NIT numbers to strings\n",
    "#na_values = ['NO CLUE', 'N/A', '0']\n",
    "#requests = pd.read_csv('YOUR.csv', na_values=na_values, dtype={'COL of numbers that are not INTEGERS': str}) THIS did not work w excel\n",
    "# then look at your list  \n",
    "# I USED dis[['STUDENT_ID']]= dis[['STUDENT_ID']].astype(str) and it WORKED\n",
    "#df['NON_INT].unique()\n",
    "#FIND ROWs with DASHES or some other incorrect value\n",
    "#rows_with_dashes = requests['NON_INT'].str.contains('-').fillna(False) giving you any thing in that col that has a dash\n",
    "#len(requests[rows_with_dashes]) \n",
    "#5 in this case 5 rows\n",
    "#then look at the rows \n",
    "#requests[rows_with_dashes] you will see the cell that have dashes in them\n",
    "# YOU COULD delete them by using requests['Incident Zip'][rows_with_dashes] = np.nan\n",
    "#or investigate long_zip_codes = requests['Incident Zip'].str.len() > 5\n",
    "#requests['Incident Zip'][long_zip_codes].unique()\n",
    "#array(['77092-2016', '55164-0737', '000000', '11549-3650', '29616-0759',\n",
    "#'35209-3114'], dtype=object) \n",
    "#these can be truncated by using str.slice  ....requests['Incident Zip'] = requests['Incident Zip'].str.slice(0, 5)\n",
    "# LOOK at a specific problem in the rows ---> requests[requests['Incident Zip'] == '00000']\n",
    "#SO SET THEM to nanzero_zips = requests['Incident Zip'] == '00000'\n",
    "#requests['Incident Zip'][zero_zips] = np.nan now VIEW AND SORT ASCENDING\n",
    "#unique_zips = requests['Incident Zip'].unique()\n",
    "#unique_zips.sort()\n",
    "#unique_zips\n",
    "#YOU CAN SORT by a certain field USE  ---> .sort('colname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CLEAN DATA PROCESS USING ZIP CODES as EXAMPLES\n",
    "#na_values = ['NO CLUE', 'N/A', '0']\n",
    "#requests = pd.read_csv('../data/311-service-requests.csv', \n",
    "                       #na_values=na_values, \n",
    "                       #dtype={'Incident Zip': str})\n",
    "#def fix_zip_codes(zips):\n",
    "    # Truncate everything to length 5 \n",
    "    #zips = zips.str.slice(0, 5)\n",
    "    \n",
    "    # Set 00000 zip codes to nan\n",
    "    #zero_zips = zips == '00000'\n",
    "    #zips[zero_zips] = np.nan\n",
    "    \n",
    "    #return zips\n",
    "#In [19]:\n",
    "#requests['Incident Zip'] = fix_zip_codes(requests['Incident Zip'])\n",
    "#In [20]:\n",
    "#requests['Incident Zip'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GET information about your dataframe\n",
    "# type (xx) will describe the object such as [OUT] pandas.core.frame.DataFrame\n",
    "\n",
    "# to get information about the type of each entry every column will be described use\n",
    "# xx.info()\n",
    "# see all the values use \n",
    "# xx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COUNT STRNG VALUES\n",
    "#data['COL_W_NON_INT'].str.upper().value_counts()\n",
    "#OR\n",
    "#len(df.index)\n",
    "#OR\n",
    "#df.shape\n",
    "#OR\n",
    "#df['col_name].count()\n",
    "#SO whats the difference between value_counts AND .count OK Value counts will give the individual values counted in a col\n",
    "#EXAMPLE \n",
    "#df3['NOTIFICATION_PLAN'].value_counts()\n",
    "#GIVES  the NAME AND QUANTITY of each value in the col\n",
    "#EA     116\n",
    "#REG     74\n",
    "#ED1      6\n",
    "#ED2      3\n",
    "#Name: NOTIFICATION_PLAN, dtype: int64\n",
    "# can also count 1s and 0s\n",
    "\n",
    "#df3['ADMITTED'].value_counts()\n",
    "#0    104\n",
    "#1     95\n",
    "#Name: ADMITTED, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select an entire column\n",
    "# xx ['Column_Name']\n",
    "## select an entire row\n",
    "# xx.loc [index or row_name]\n",
    "## only show the first 5 rows transposed\n",
    "#xx[:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show the top or bot 5 of the dataframe\n",
    "#xx.head()\n",
    "#xx.tail()\n",
    "# Show the columns the cols will display as a list\n",
    "# xx.columns\n",
    "# Select multiple cols by name\n",
    "#select  multiple cols by label\n",
    "# xx.loc[:,['COL_NAME_1','COl_NAME_2']] this will give you  all rows because of  _:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will read a csv file as text no cols each line is read as text with no dataframe DOS or ExcelCSV same \n",
    "#import pandas as pd\n",
    "#import csv\n",
    "#reader = csv.reader(open ('C:/Users/mjc341/desktop/TESTPY/PYtest1.csv',newline=''),delimiter=',',quotechar='|')\n",
    "#for row in reader:\n",
    "        #print (','.join(row))\n",
    "    #Have to save as csv DOS file not excel csv or PY cant read it so EXCEL csv has to be saved as csv DOS\n",
    "# to find the type enter  type (df) your answer will be ... pandas.core.frame.DataFrame\n",
    "# to select a row # select an entire row\n",
    "#df.loc ['row_name']\n",
    "# only show the first 5 rows transposed\n",
    "#df2[:5].T\n",
    "# get information about the type of each entry\n",
    "#df2.info()\n",
    "# get info about the index \n",
    "#df2.index\n",
    "#RangeIndex(start=0, stop=99, step=1)\n",
    "#select  multiple cols by label\n",
    "#df2.loc[0:4,['APPLICANT_ID','APPLICANT_NAME']]\n",
    "#explicitly slice rows\n",
    "#df2.iloc[1:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONE way to determine if there are duplicate values in a col USE for KEY INDEX dups\n",
    "#if xx['COL_NAME_'].size == dfx['COL_NAME'].unique().size:\n",
    "    #print(\"Unique\")\n",
    "#else:\n",
    "    #print(\"Not unique\")\n",
    "    \n",
    "# YOU can also create a col that track the dups\n",
    " #dfx['dupes'] = dfx.duplicated('APPLICANT_ID',) [dupes] is the new col in the df will have boolean True or False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COUNT Values\n",
    "#xx['COLUMN_NAME'].count()  ALSO min, max,sum, median\n",
    "#Get descriptive statistics for a specified column\n",
    "\n",
    "#Statistics I have tested this works great\n",
    "#dfx.col_name.describe()  for integers values gives mean std min max quartiles\n",
    "\n",
    "#Group data and obtain the mean values   grpagg = ver.groupby('COLUMN_NAME').aggregate(np.mean)\n",
    "#grpagg\n",
    "\n",
    "#Group data and get the sums\n",
    "#grpsum = ver.groupby('COLUMN_name').aggregate(np.sum)\n",
    "#grpsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MELT Operation\n",
    "# pd.melt(dfx, id_vars=['APPLICANT_ID'], value_vars=['APPLICANT_ETHNIC_GROUP']).head(5) this will melt one col into another by sorting the \n",
    "#1st col values by the 2nd col values this will give us each applicant ID by ethnic group the ID var by the value Var\n",
    "\n",
    "#ANother way to sort is by creating a crosstab\n",
    "#pd.crosstab(dfx['APPLICANT_ID'],dfx['APPLICANT_ETHNIC_GROUP']).head(10) Ethnic group will be the header and Applicant ID will be the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a subset of a certain value\n",
    "# dfxsubset = dfx[(dfx['NET_DEPOSITS']== 1)] this will give all the rows where NET DEP  == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#demog1= dfx.groupby(['APPLICANT_ETHNIC_GROUP','GENDER']).sum()  Remove Applicant ID col and use sum of Applicant count col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####OMG CHANGE THE INDEX VALUE df1= df.set_index('col_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get 1 column \n",
    "#dis['ACAD_GROUP'] #first 5 rows all columns\n",
    "#dis[:5]\n",
    "#combine 8 and 10 IN THE SAME LINE to get 1st 5 rows of  certain col\n",
    "#dis['ACAD_GROUP'][:5]\n",
    "#Select a subset of cols and first 10 rows\n",
    "#dis [['ACAD_GROUP','IGRANT']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a count of values by col name\n",
    "#dis['ACAD_GROUP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PLT style\n",
    "# plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a value in a col = to the desired value To get the noise complaints, \n",
    "#we need to find the rows where the \"Complaint Type\" column is \"Noise - Street/Sidewalk\". \n",
    "#noise_complaints = complaints[complaints['Complaint Type'] == \"Noise - Street/Sidewalk\"]\n",
    "#noise_complaints[:3]\n",
    "\n",
    "#You can also combine more than one condition with the & operator like this:\n",
    "#is_noise = complaints['Complaint Type'] == \"Noise - Street/Sidewalk\"\n",
    "#in_brooklyn = complaints['Borough'] == \"BROOKLYN\"\n",
    "#complaints[is_noise & in_brooklyn][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inp = input('Enter Fahrenheit Temperature:')\n",
    "#try:\n",
    "#fahr = float(inp)\n",
    "#cel = (fahr - 32.0) * 5.0 / 9.0\n",
    "#print (cel)\n",
    "#except:\n",
    "#print ('Thats not a number')\n",
    "    \n",
    "#Enter Fahrenheit Temperature:forty\n",
    "#Thats not a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range (10):\n",
    "#x = random.random()\n",
    "#print (x)\n",
    "\n",
    "#for i in range (10):\n",
    "#x = random.randint(5,100)\n",
    "\n",
    "#print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GROUPING \n",
    "#Get descriptive statistics for a specified column\n",
    "#dfx.col_name.describe()  for integers values gives mean std min max quartiles\n",
    "#Group data and obtain the mean values   grpagg = ver.groupby('purchaser_type_name').aggregate(np.mean)\n",
    "#grpagg\n",
    "#Group data and get the sums\n",
    "#grpsum = ver.groupby('purchaser_type_name').aggregate(np.sum)\n",
    "#grpsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MUNGE\n",
    "#CREATE CATEGORICAL ranges\n",
    "#incomeranges =pd.cut(df['value_col'], 14) the #14 is the number of ranges you want\n",
    "\n",
    "#GET the UNIQUE values of a col by name gives you an array\n",
    "#df['COL_NAME'].unique()\n",
    "\n",
    "#GET a binary indicator\n",
    "#df.ix[:,'COL A'] == \"VALUE you are checking\"\n",
    "\n",
    "#obtain the data types in the cols\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#combine conditions in a dataset \n",
    "#df2 = df['name_of_Col 1'] == \"aValueInCol 1\"\n",
    "#df3 = df['name of Col 2'] == \"Avaluein Col 2\"  you will get the entire row all of the other values in that row are included\n",
    "#df[is_noise & in_brooklyn][:5] return the 1st 5 rows of df2 anf df 3 combined and replace the original df with that df if you \n",
    "#wanted to keep the original df then youd name the new combined df as df4 or something if you only want a few of the columns from \n",
    "#those row then use:complaints[is_noise & in_brooklyn][['Complaint Type', 'Borough', 'Created Date', 'Descriptor']][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A column in Pandas is a SERIES \n",
    "import pandas as pd\n",
    "pd.Series([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2,3]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pandas Series are internally numpy arrays. If you add .values to the end of any Series, you'll get its internal numpy array\n",
    "# that is why you can get boolian selection reponses out of columns because NP arrys have bool quality\n",
    "arr=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr!=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[arr!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if Py 3 is using integer division which means every divded int will be rounded off to the nearest whole number\n",
    "#ifso then you have to change your values to floats and then you can get decimal results to divide then\n",
    "#noise_complaint_counts = noise_complaints['Borough'].value_counts()\n",
    "#complaint_counts = complaints['Borough'].value_counts()\n",
    "#noise_complaint_counts / complaint_counts.astype(float)\n",
    "#[in]#noise_complaint_counts / complaint_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATAFRAME .groupby() is like SQL\n",
    "#In this case, berri_bikes.groupby('weekday').aggregate(sum) \n",
    "#weekday_counts = berri_bikes.groupby('weekday').aggregate(sum)\n",
    "#weekday_counts this will display the results\n",
    "#means \"Group the rows by weekday and then add up all the values with the same weekday\".\n",
    "# so group the rows by 'ethnicity' and ad up all the values with the same ethnicicty/school/GPA etc\n",
    "# you can rename the index in this example the weekdays are number values to replace that witht he name of the day use \n",
    "#weekday_counts.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "# THEN plot it weekday_counts.plot(kind='bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO combine files into a dataframe STACKED  if all cols are the same then use dfnamne = pd.concat(name of file(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
